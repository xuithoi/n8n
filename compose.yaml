services:
  svr_localai:
    image: localai/localai:latest-gpu-nvidia-cuda-12
    container_name: cont_localai
    restart: always
    ports:
      - "8080:8080"
    volumes:
      - /root/vol_localai:/build/models
    environment:
      - MODELS_PATH=/build/models
    command:
      - dreamshaper
      - llama-3.2-3b-instruct:q8_0
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            device_ids: ['0']
            capabilities: [gpu]
  svr_n8n:
    image: n8nio/n8n
    container_name: cont_n8n
    environment:
      - N8N_SECURE_COOKIE=false
      - GENERIC_TIMEZONE=Asia/Ho_Chi_Minh
      - N8N_EDITOR_BASE_URL=${EXTERNAL_IP}
      - WEBHOOK_URL=${EXTERNAL_IP}
      - N8N_DEFAULT_BINARY_DATA_MODE=filesystem
      - N8N_MCP_ENABLED=true
      - N8N_MCP_MODE=server
      - N8N_HOST=${EXTERNAL_IP}
      - WEBHOOK_URL=${EXTERNAL_IP}
    ports:
      - "80:5678"
    volumes:
      - ${CURR_DIR}/vol_n8n:/home/node/.n8n
